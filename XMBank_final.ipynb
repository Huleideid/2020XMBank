{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K03WTDaTNq1e"
   },
   "source": [
    "### 导入相关库和封装函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K5fIv3ugNq1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def drop_correlated_col(df, cutoff):\n",
    "    # 筛选高度相关特征\n",
    "    def filter_corr(corr, cutoff=0.9):\n",
    "        cols = []\n",
    "        for i,j in feature_group:\n",
    "            if corr.loc[i, j] > cutoff:\n",
    "                print(i,j,corr.loc[i, j])\n",
    "                i_avg = corr[i][corr[i] != 1].mean()\n",
    "                j_avg = corr[j][corr[j] != 1].mean()\n",
    "                if i_avg >= j_avg:\n",
    "                    cols.append(i)\n",
    "                else:\n",
    "                    cols.append(j)\n",
    "        return set(cols)\n",
    "\n",
    "    corr = df.corr()\n",
    "    feature_group = list(itertools.combinations(corr.columns, 2))\n",
    "    drop_cols = filter_corr(corr, cutoff)\n",
    "    print(list(drop_cols))\n",
    "    df.drop(list(drop_cols),inplace = True, axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def statistics_feature(train, test, df, agg_stat, cutoff=0.96, fillna=1):\n",
    "\n",
    "    # 数据处理\n",
    "    group_df3 = df[(df['mon']<=9)&(df['mon']>=7)].groupby(['cust_no']).agg(agg_stat)\n",
    "    group_df3.columns = [f[0]+'_'+f[1] for f in group_df3.columns]\n",
    "    group_df3.reset_index(inplace=True)\n",
    "    group_df3['season'] = 3\n",
    "\n",
    "    group_df4 = df[(df['mon']<=12)&(df['mon']>=10)].groupby(['cust_no']).agg(agg_stat)\n",
    "    col = [f[0]+'_'+f[1] for f in group_df4.columns]\n",
    "    group_df4.columns = [f[0]+'_'+f[1] for f in group_df4.columns]\n",
    "    group_df4.reset_index(inplace=True)\n",
    "    group_df4['season'] = 4\n",
    "\n",
    "    group_df1 = df[(df['mon']<=15)&(df['mon']>=13)].groupby(['cust_no']).agg(agg_stat)\n",
    "    group_df1.columns = [f[0]+'_'+f[1] for f in group_df1.columns]\n",
    "    group_df1.reset_index(inplace=True)\n",
    "    group_df1['season'] = 5\n",
    "                           \n",
    "    stat = pd.concat([pd.concat([group_df3, group_df4], axis=0, ignore_index=True), group_df1], axis=0, ignore_index=True)\n",
    "    stat.to_pickle('stat.pkl')\n",
    "\n",
    "    del group_df3, group_df4, group_df1      \n",
    "    \n",
    "    # 剔除高度相关特征\n",
    "    stat = drop_correlated_col(stat, cutoff)\n",
    "    \n",
    "    # 将特征合并进去         \n",
    "    train, test = merge_feat(train, test, stat, fillna)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def merge_feat(train, test, df, fillna):\n",
    "    \n",
    "    tmp = df[df['season']==3].copy()\n",
    "    del tmp['season']\n",
    "    tmp.columns = ['cust_no'] +  [f+'_1' for f in tmp.columns[1:]]\n",
    "    col_1 = [f for f in tmp.columns[1:]]\n",
    "    train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "    if fillna==1:\n",
    "        train[col_1] = train[col_1].fillna(value=0)\n",
    "#     train[col_1].fillna(value=0, inplace=True)\n",
    "\n",
    "\n",
    "    tmp = df[df['season']==4].copy()\n",
    "    del tmp['season']\n",
    "    tmp.columns = ['cust_no'] +  [f+'_2' for f in tmp.columns[1:]]\n",
    "    col_2 = [f for f in tmp.columns[1:]]\n",
    "    train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "    if fillna==1:\n",
    "        train[col_2] = train[col_2].fillna(value=0)\n",
    "\n",
    "\n",
    "    for i,j  in zip(col_1, col_2):\n",
    "        train[j+'_'+i] = train[j] - train[i]\n",
    "\n",
    "    tmp = df[df['season']==4].copy()\n",
    "    del tmp['season']\n",
    "    tmp.columns = ['cust_no'] +  [f+'_1' for f in tmp.columns[1:]]\n",
    "    col_1 = [f for f in tmp.columns[1:]]\n",
    "    test = test.merge(tmp, on=['cust_no'], how='left')\n",
    "    if fillna==1:\n",
    "        test[col_1] = test[col_1].fillna(value=0)\n",
    "\n",
    "\n",
    "    tmp = df[df['season']==5].copy()\n",
    "    del tmp['season']\n",
    "    tmp.columns = ['cust_no'] +  [f+'_2' for f in tmp.columns[1:]]\n",
    "    col_2 = [f for f in tmp.columns[1:]]\n",
    "    test = test.merge(tmp, on=['cust_no'], how='left')\n",
    "    if fillna==1:\n",
    "        test[col_2] = test[col_2].fillna(value=0)\n",
    "\n",
    "    for i,j  in zip(col_1, col_2):\n",
    "        test[j+'_'+i] = test[j] - test[i]\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "def get_Fluction_Feature(df_fea, train, test, f):\n",
    "    df_fea = df_fea[~df_fea['cust_no'].isnull()] #去除id为空的行\n",
    "    stat = pd.DataFrame(df_fea[['cust_no']].drop_duplicates())#去除重复值\n",
    "    for i in range(7,16):\n",
    "        tmp = df_fea[(df_fea['mon']==i)][['cust_no', f]].copy()\n",
    "        stat = stat.merge(tmp, on=['cust_no'], how='left')\n",
    "        print(stat.shape)\n",
    "    stat.fillna(value=0, inplace=True)\n",
    "    stat.columns =['cust_no'] + [f + '_' + str(i) for i in range(7, 16)]\n",
    "    # 季度内波动\n",
    "    stat[f+'_3s'] = stat[f +'_9'] - stat[f +'_7']\n",
    "    stat[f+'_4s'] = stat[f +'_12'] - stat[f +'_10']\n",
    "    stat[f+'_5s'] = stat[f +'_15'] - stat[f +'_13']\n",
    "    # 季度间波动\n",
    "    stat[f+'_34s'] = stat[f +'_12'] - stat[f +'_9']\n",
    "    stat[f+'_45s'] = stat[f +'_15'] - stat[f +'_12']\n",
    "\n",
    "\n",
    "    tmp = stat[['cust_no', f+'_9', f+'_12', f+'_3s', f+'_4s', f+'_34s']].copy()\n",
    "    tmp.columns = ['cust_no'] + [f+'_'+str(i) for i in range(1,6)]\n",
    "    train = train.merge(tmp, on=['cust_no'], how='left')\n",
    "    del tmp\n",
    "\n",
    "    tmp = stat[['cust_no',f+'_12', f+'_15', f+'_4s', f+'_5s',f+'_45s']].copy()\n",
    "    tmp.columns = ['cust_no'] + [f+'_'+str(i) for i in range(1,6)]\n",
    "    test = test.merge(tmp, on=['cust_no'], how='left')\n",
    "    del tmp\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aum_m10.csv\n",
      "aum_m10.csv.baiduyun.uploading.cfg\n",
      "aum_m11.csv\n",
      "aum_m11.csv.baiduyun.uploading.cfg\n",
      "aum_m12.csv\n",
      "aum_m12.csv.baiduyun.uploading.cfg\n",
      "aum_m7.csv\n",
      "aum_m7.csv.baiduyun.uploading.cfg\n",
      "aum_m8.csv\n",
      "aum_m8.csv.baiduyun.uploading.cfg\n",
      "aum_m9.csv\n",
      "aum_m9.csv.baiduyun.uploading.cfg\n",
      "aum_m1.csv\n",
      "aum_m2.csv\n",
      "aum_m3.csv\n",
      "behavior_m10.csv\n",
      "behavior_m10.csv.baiduyun.uploading.cfg\n",
      "behavior_m11.csv\n",
      "behavior_m11.csv.baiduyun.uploading.cfg\n",
      "behavior_m12.csv\n",
      "behavior_m12.csv.baiduyun.uploading.cfg\n",
      "behavior_m7.csv\n",
      "behavior_m7.csv.baiduyun.uploading.cfg\n",
      "behavior_m8.csv\n",
      "behavior_m8.csv.baiduyun.uploading.cfg\n",
      "behavior_m9.csv\n",
      "behavior_m9.csv.baiduyun.uploading.cfg\n",
      "behavior_m1.csv\n",
      "behavior_m2.csv\n",
      "behavior_m3.csv\n",
      "behavior_m3.csv.baiduyun.uploading.cfg\n",
      "big_event_Q3.csv\n",
      "big_event_Q3.csv.baiduyun.uploading.cfg\n",
      "big_event_Q4.csv\n",
      "big_event_Q4.csv.baiduyun.uploading.cfg\n",
      "big_event_Q1.csv\n",
      "big_event_Q1.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m10.csv\n",
      "cunkuan_m10.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m11.csv\n",
      "cunkuan_m11.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m12.csv\n",
      "cunkuan_m12.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m7.csv\n",
      "cunkuan_m7.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m8.csv\n",
      "cunkuan_m8.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m9.csv\n",
      "cunkuan_m9.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m1.csv\n",
      "cunkuan_m1.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m2.csv\n",
      "cunkuan_m2.csv.baiduyun.uploading.cfg\n",
      "cunkuan_m3.csv\n",
      "cunkuan_m3.csv.baiduyun.uploading.cfg\n"
     ]
    }
   ],
   "source": [
    "os.getcwd() \n",
    "os.chdir('./data')\n",
    "\n",
    "### 读取数据\n",
    "y_Q3_3 = pd.read_csv('./y_train_3/y_Q3_3.csv')\n",
    "y_Q4_3 = pd.read_csv('./y_train_3/y_Q4_3.csv')\n",
    "\n",
    "aum_fils = os.listdir('x_train/aum_train/')+os.listdir('x_test/aum_test/')\n",
    "aum = []\n",
    "for f in aum_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/aum_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/aum_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    aum.append(tmp)\n",
    "aum = pd.concat(aum, axis=0, ignore_index=True)\n",
    "\n",
    "behavior_fils = os.listdir('x_train/behavior_train/')+os.listdir('x_test/behavior_test/')\n",
    "behavior = []\n",
    "for f in behavior_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/behavior_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/behavior_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    behavior.append(tmp)\n",
    "behavior = pd.concat(behavior, axis=0, ignore_index=True)\n",
    "\n",
    "event_fils = os.listdir('x_train/big_event_train/')+os.listdir('x_test/big_event_test/')\n",
    "event = []\n",
    "for f in event_fils:\n",
    "    print(f)\n",
    "    season = int((f.split('.')[0]).split('_')[-1].replace('Q', ''))\n",
    "    if season>=3:\n",
    "        tmp = pd.read_csv('x_train/big_event_train/'+f)\n",
    "        tmp['season'] = season\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/big_event_test/'+f)\n",
    "        tmp['season'] = season + 4\n",
    "    \n",
    "    event.append(tmp)\n",
    "event = pd.concat(event, axis=0, ignore_index=True)\n",
    "del event['c']\n",
    "\n",
    "cunkuan_fils = os.listdir('x_train/cunkuan_train/')+os.listdir('x_test/cunkuan_test/')\n",
    "cunkuan = []\n",
    "for f in cunkuan_fils:\n",
    "    print(f)\n",
    "    mon = int((f.split('.')[0]).split('_')[-1].replace('m', ''))\n",
    "    if mon>=7:\n",
    "        tmp = pd.read_csv('x_train/cunkuan_train/'+f)\n",
    "        tmp['mon'] = mon\n",
    "    else:\n",
    "        tmp = pd.read_csv('x_test/cunkuan_test/'+f)\n",
    "        tmp['mon'] = mon+12\n",
    "    cunkuan.append(tmp)\n",
    "cunkuan = pd.concat(cunkuan, axis=0, ignore_index=True)\n",
    "\n",
    "cust_avli_Q3 = pd.read_csv('./x_train/cust_avli_Q3.csv')\n",
    "cust_avli_Q4 = pd.read_csv('./x_train/cust_avli_Q4.csv')\n",
    "cust_info_Q3 = pd.read_csv('x_train/cust_info_q3.csv')\n",
    "cust_info_Q4 = pd.read_csv('x_train/cust_info_q4.csv')\n",
    "cust_avli_Q1 = pd.read_csv('x_test/cust_avli_Q1.csv')\n",
    "cust_info_Q1 = pd.read_csv('x_test/cust_info_q1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpzkH-62Nq1i"
   },
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g4M4E5NNq1i"
   },
   "source": [
    "第一组特征很自然的想到用户历史的label，例如在预测季度4的用户时，使用用户在季度3的label作为特征。可以简单看到这个特征的kappa值可以达到0.238+。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_no</th>\n",
       "      <th>bef_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3b9b4615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x3b9ae61b</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x3b9add69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x3b9b3601</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x3b9b2599</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76717</th>\n",
       "      <td>0xb2d69017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76718</th>\n",
       "      <td>0xb2d68153</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76719</th>\n",
       "      <td>0xb2d5bba1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76720</th>\n",
       "      <td>0xb2d61b9b</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76721</th>\n",
       "      <td>0xb2d70079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cust_no  bef_label\n",
       "0      0x3b9b4615        0.0\n",
       "1      0x3b9ae61b        1.0\n",
       "2      0x3b9add69        0.0\n",
       "3      0x3b9b3601        0.0\n",
       "4      0x3b9b2599        0.0\n",
       "...           ...        ...\n",
       "76717  0xb2d69017        0.0\n",
       "76718  0xb2d68153        1.0\n",
       "76719  0xb2d5bba1        1.0\n",
       "76720  0xb2d61b9b        1.0\n",
       "76721  0xb2d70079        0.0\n",
       "\n",
       "[76722 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = y_Q4_3.copy()\n",
    "y_Q3_3 = y_Q3_3.rename(columns={'label': 'bef_label'})\n",
    "train = train.merge(y_Q3_3, on=['cust_no'], how='left').copy()\n",
    "\n",
    "test = cust_avli_Q1.copy()\n",
    "y_Q4_3 = y_Q4_3.rename(columns={'label': 'bef_label'})\n",
    "test = test.merge(y_Q4_3, on=['cust_no'], how='left')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户特征属性处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKkgJIrBNq1i",
    "outputId": "dea492da-3c9b-42b6-d451-a7aa94b2b2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76170, 13) (76722, 12)\n",
      "I3_x\n",
      "I5\n",
      "I10\n",
      "I13\n",
      "I14\n",
      "I3_y\n"
     ]
    }
   ],
   "source": [
    "# 根据EDA去掉一部分的列\n",
    "cust_info_Q4.drop(columns = ['I1','I2','I4','I7','I8','I9','I12','I15','I17','I18','I19'], inplace=True)\n",
    "cust_info_Q3.drop(columns = ['I1','I2','I4','I7','I8','I9','I12','I15','I17','I18','I19'], inplace=True)\n",
    "cust_info_Q1.drop(columns = ['I1','I2','I4','I7','I8','I9','I12','I15','I17','I18','I19'], inplace=True)\n",
    "train = train.merge(cust_info_Q4, on=['cust_no'], how='left')\n",
    "train = train.merge(cust_info_Q3[['cust_no','I3']], on=['cust_no'], how='left')\n",
    "test = test.merge(cust_info_Q1, on=['cust_no'], how='left')\n",
    "test = test.merge(cust_info_Q4[['cust_no','I3']], on=['cust_no'], how='left')\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "### 特征编码\n",
    "for col in [f for f in train.select_dtypes('object').columns if f not in ['label', 'cust_no']]:\n",
    "    print(col)\n",
    "    train[col].fillna(train[col].mode()[0], inplace=True)### 缺失值处理·\n",
    "    test[col].fillna(train[col].mode()[0], inplace=True)### 缺失值处理·\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pd.concat([train[[col]], test[[col]]], axis=0, ignore_index=True))\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 存款特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1_max C1_sum 0.9722382981454611\n",
      "C1_min C1_sum 0.9638184487079668\n",
      "C1_min C1_last 0.9721794830610042\n",
      "C1_sum C1_last 0.967627208540242\n",
      "['C1_last', 'C1_sum']\n"
     ]
    }
   ],
   "source": [
    "cunkuan = cunkuan.sort_values(by=['cust_no', 'mon']).reset_index(drop=True)\n",
    "cunkuan_agg_stat = {'C1': ['max', 'min', 'sum', 'last'],\n",
    "#             'C2': ['max', 'min', 'sum', 'last'],\n",
    "           }\n",
    "train, test = statistics_feature(train, test, cunkuan, cunkuan_agg_stat, cutoff=0.96)\n",
    "# cunkuan_stat = pd.read_pickle('cunkuan_stat.pkl')\n",
    "# cunkuan_stat = drop_correlated_col(cunkuan_stat, cutoff = 0.96)\n",
    "# train, test = merge_feat(train, test, cunkuan_stat, fillna=1)\n",
    "# train, test =  get_Fluction_Feature(cunkuan, train, test, 'C1')\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 金额特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_sum X1_last 0.9686445268910792\n",
      "X2_sum X2_max 0.9717797933901305\n",
      "X2_min X2_last 0.9831454095524654\n",
      "X7_sum X7_last 0.9723667616496774\n",
      "X_sum_sum X_sum_min 0.9672674573232287\n",
      "X_sum_sum X_sum_max 0.9732399867084586\n",
      "X_sum_sum X_sum_last 0.9706362970747083\n",
      "['X1_sum', 'X_sum_max', 'X2_last', 'X2_sum', 'X7_last', 'X_sum_sum']\n",
      "(659624, 2)\n",
      "(659624, 3)\n",
      "(659624, 4)\n",
      "(659624, 5)\n",
      "(659624, 6)\n",
      "(659624, 7)\n",
      "(659624, 8)\n",
      "(659624, 9)\n",
      "(659624, 10)\n",
      "(659624, 2)\n",
      "(659624, 3)\n",
      "(659624, 4)\n",
      "(659624, 5)\n",
      "(659624, 6)\n",
      "(659624, 7)\n",
      "(659624, 8)\n",
      "(659624, 9)\n",
      "(659624, 10)\n",
      "(659624, 2)\n",
      "(659624, 3)\n",
      "(659624, 4)\n",
      "(659624, 5)\n",
      "(659624, 6)\n",
      "(659624, 7)\n",
      "(659624, 8)\n",
      "(659624, 9)\n",
      "(659624, 10)\n",
      "(76170, 106) (76722, 105)\n"
     ]
    }
   ],
   "source": [
    "X_cols = [f for f in aum.columns if f.startswith('X') and f not in ['X7']]\n",
    "aum['X_sum'] = aum[X_cols].sum(axis=1) - aum['X7']\n",
    "aum['X_fuzailv'] = aum['X7']/aum['X_sum'] \n",
    "aum['X_siqilv'] = aum['X2']/aum['X_sum'] \n",
    "aum['X_huoqilv'] = aum['X3']/aum['X_sum'] \n",
    "aum_agg_stat = {'X1': [ 'sum', 'last'],\n",
    "                'X2': [ 'sum', 'min', 'max', 'last'],\n",
    "                'X3': [ 'sum', 'min', 'max', 'last'],\n",
    "                'X4': [ 'sum', 'last'],\n",
    "                'X5': [ 'sum', 'last'],\n",
    "                'X6': [ 'sum', 'last'],\n",
    "                'X7': [ 'sum', 'last'],\n",
    "                'X8': [ 'sum', 'last'],\n",
    "                'X_sum': [ 'sum', 'min', 'max', 'last'],\n",
    "                'X_fuzailv': [ 'sum', 'last'],\n",
    "                'X_siqilv': [ 'sum', 'last'],\n",
    "                'X_huoqilv': [ 'sum', 'last'],\n",
    "           }\n",
    "train, test = statistics_feature(train, test, aum, aum_agg_stat, cutoff=0.96)\n",
    "train, test =  get_Fluction_Feature(aum, train, test, 'X_sum')\n",
    "train, test =  get_Fluction_Feature(aum, train, test, 'X2')\n",
    "train, test =  get_Fluction_Feature(aum, train, test, 'X3')\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行为特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B5/B3_max B5/B3_min 0.9968893511806203\n",
      "B5/B3_max B5/B3_sum 0.9999999985612688\n",
      "B5/B3_min B5/B3_sum 0.9968893437084174\n",
      "['B5/B3_sum', 'B5/B3_max']\n"
     ]
    }
   ],
   "source": [
    "behavior['B5-B3'] = behavior['B5'] - behavior['B3']\n",
    "behavior['B5/B3'] = behavior['B5'] / behavior['B3']\n",
    "behavior.loc[((behavior['mon']==9)),'B6'] =  (pd.to_datetime('2019-10-01 00:00:00') -pd.to_datetime(behavior.loc[((behavior['mon']==9)),'B6'])).dt.days\n",
    "behavior.loc[((behavior['mon']==12)),'B6'] =  (pd.to_datetime('2020-01-01 00:00:00')-pd.to_datetime(behavior.loc[((behavior['mon']==12)),'B6'])).dt.days\n",
    "behavior.loc[((behavior['mon']==15)),'B6'] =  (pd.to_datetime('2020-04-01 00:00:00') -pd.to_datetime(behavior.loc[((behavior['mon']==15)),'B6'])).dt.days\n",
    "behavior_agg_stat = {'B5-B3': ['max', 'min', 'sum'],\n",
    "            'B5/B3': ['max', 'min', 'sum'],\n",
    "            'B1': ['max', 'min', 'sum'],\n",
    "            'B2': ['max', 'min', 'sum'],\n",
    "            'B3': ['max', 'min', 'sum'],\n",
    "            'B4': ['max', 'min', 'sum'],\n",
    "            'B5': ['max', 'min', 'sum'],\n",
    "            'B6': ['last'],\n",
    "            'B7': ['last'],\n",
    "           }\n",
    "\n",
    "train, test = statistics_feature(train, test, behavior, behavior_agg_stat, cutoff=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3/B2_min B3/B2_last 0.9714778417386742\n",
      "B5/B3_max B5/B3_min 0.9968893511806203\n",
      "B5/B3_max B5/B3_std 0.99752284106521\n",
      "B5/B3_max B5/B3_sum 0.9999999985612688\n",
      "B5/B3_max B5/B3_last 0.9969164322151617\n",
      "B5/B3_min B5/B3_sum 0.9968893437084174\n",
      "B5/B3_min B5/B3_last 0.9999728105501697\n",
      "B5/B3_std B5/B3_sum 0.997522396716309\n",
      "B5/B3_sum B5/B3_last 0.9969164276516074\n",
      "['B5/B3_last', 'B3/B2_last', 'B5/B3_sum', 'B5/B3_max']\n",
      "(76170, 400) (76722, 399)\n"
     ]
    }
   ],
   "source": [
    "# behavior_stat = pd.read_pickle('behavior_stat.pkl')\n",
    "# behavior_stat = drop_correlated_col(behavior_stat, cutoff = 0.97)\n",
    "# train, test = merge_feat(train, test, behavior_stat, fillna=0)\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1\n",
      "E2\n",
      "E4\n",
      "E5\n",
      "E6\n",
      "E7\n",
      "E8\n",
      "E9\n",
      "E10\n",
      "E12\n",
      "E13\n",
      "E14\n",
      "E16\n",
      "E18\n"
     ]
    }
   ],
   "source": [
    "del event['E3'], event['E11']\n",
    "## 现将异常值替换成nan\n",
    "event.loc[pd.to_datetime(event['E1'])<pd.to_datetime('1949-10-01 00:00:00'), 'E1']= np.nan\n",
    "event.loc[pd.to_datetime(event['E8'])<pd.to_datetime('1949-10-01 00:00:00'), 'E8']= np.nan\n",
    "\n",
    "## 再用E2填充E1\n",
    "event.loc[event['E1'].isnull()&event['E2'].notnull(), 'E1'] = event.loc[event['E1'].isnull()&event['E2'].notnull(), 'E2']\n",
    "\n",
    "## 现转换成时间数据格式\n",
    "E_cols = [f for f in event.columns if f.startswith('E')]\n",
    "for col in E_cols:\n",
    "    if col not in [ 'E15', 'E17']:\n",
    "        event[col] = pd.to_datetime(event[col])\n",
    "\n",
    "def get_Dateencoder(x, flag):\n",
    "    if flag==1:\n",
    "        #  当季度   \n",
    "        if x > pd.to_datetime('2019-10-01 00:00:00'): return 1\n",
    "         #  上一季度  \n",
    "        elif  x > pd.to_datetime('2019-07-01 00:00:00'): return 2\n",
    "         #  上上季度  \n",
    "        elif  x > pd.to_datetime('2019-04-01 00:00:00'): return 3\n",
    "        #  上上上季度 \n",
    "        elif  x > pd.to_datetime('2019-01-01 00:00:00'): return 4\n",
    "        #  长期\n",
    "        return 5\n",
    "    elif flag==0:\n",
    "        #  当季度  \n",
    "        if x > pd.to_datetime('2020-01-01 00:00:00'): return 1    \n",
    "        #  上一季度  \n",
    "        elif x > pd.to_datetime('2019-10-01 00:00:00'): return 2\n",
    "         #  上上季度  \n",
    "        elif  x > pd.to_datetime('2019-07-01 00:00:00'): return 3\n",
    "        #  上上上季度   \n",
    "        elif  x > pd.to_datetime('2019-04-01 00:00:00'): return 4\n",
    "        #  长期\n",
    "        return 5\n",
    "    \n",
    "# train = pd.DataFrame(cust_avli_Q4['cust_no'])\n",
    "train = train.merge(event.loc[event['season']==4], how='inner', on='cust_no')\n",
    "train.head()\n",
    "del train['season']\n",
    "\n",
    "# test = pd.DataFrame(cust_avli_Q1['cust_no'])\n",
    "test = test.merge(event.loc[event['season']==5], how='inner', on='cust_no')\n",
    "del test['season']\n",
    "\n",
    "\n",
    "E_cols = [f for f in event.columns if f.startswith('E')]\n",
    "for col in E_cols:\n",
    "    if col not in [ 'E15', 'E17']:\n",
    "        print(col)\n",
    "        train[col] = train[col].apply(lambda x: get_Dateencoder(x, 1))\n",
    "        test[col] = test[col].apply(lambda x: get_Dateencoder(x, 0))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['I10', 'I18', 'C3_std_1', 'C2_skew_2_C2_skew_1', 'C3_min_2_C3_min_1',\n",
      "       'X2_min_1', 'X2_std_1', 'X2_sum_1', 'X3_max_1', 'X3_std_1', 'X3_sum_1',\n",
      "       'X5_max_1', 'X5_min_1', 'X5_std_1', 'X5_sum_1', 'X5_last_1', 'X6_std_1',\n",
      "       'X6_skew_1', 'X8_std_1', 'X_num_skew_1', 'X2_min_2', 'X2_sum_2',\n",
      "       'X2_skew_2', 'X5_min_2', 'X5_sum_2', 'X6_max_2', 'X6_min_2', 'X6_std_2',\n",
      "       'X6_sum_2', 'X6_last_2', 'X8_skew_2', 'X1_min_2_X1_min_1',\n",
      "       'X1_std_2_X1_std_1', 'X1_sum_2_X1_sum_1', 'X1_skew_2_X1_skew_1',\n",
      "       'X3_min_2_X3_min_1', 'X3_last_2_X3_last_1', 'X6_std_2_X6_std_1',\n",
      "       'X7_std_2_X7_std_1', 'X7_skew_2_X7_skew_1', 'X8_std_2_X8_std_1',\n",
      "       'X_num_std_2_X_num_std_1', 'B5-B3_max_1', 'B5-B3_min_1', 'B5-B3_std_1',\n",
      "       'B5-B3_sum_1', 'B4-B2_sum_1', 'B4-B2_last_1', 'B5/B4_std_1',\n",
      "       'B4/B2_skew_1', 'B5/B3_min_1', 'B2_std_1', 'B3_min_1', 'B3_last_1',\n",
      "       'B5_min_1', 'B5_last_1', 'B5-B3_sum_2', 'B5-B3_last_2', 'B5/B4_skew_2',\n",
      "       'B5/B3_min_2', 'B1_std_2', 'B1_skew_2', 'B5_skew_2',\n",
      "       'B5-B3_sum_2_B5-B3_sum_1', 'B3/B2_min_2_B3/B2_min_1',\n",
      "       'B5/B4_min_2_B5/B4_min_1', 'B5/B4_std_2_B5/B4_std_1',\n",
      "       'B5/B4_last_2_B5/B4_last_1', 'B5/B3_min_2_B5/B3_min_1',\n",
      "       'B1_max_2_B1_max_1', 'B2_sum_2_B2_sum_1', 'B3_min_2_B3_min_1',\n",
      "       'B4_last_2_B4_last_1'],\n",
      "      dtype='object')\n",
      "(76170, 343) (76722, 342)\n"
     ]
    }
   ],
   "source": [
    "# drop_cols = train.corr()['label'][abs(train.corr()['label'])<0.01].index\n",
    "# print(drop_cols)\n",
    "# test.drop(drop_cols,axis=1,inplace=True)\n",
    "# train.drop(drop_cols,axis=1,inplace=True)\n",
    "# print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust_no</th>\n",
       "      <th>label</th>\n",
       "      <th>bef_label</th>\n",
       "      <th>I3_x</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I13</th>\n",
       "      <th>I14</th>\n",
       "      <th>...</th>\n",
       "      <th>E9</th>\n",
       "      <th>E10</th>\n",
       "      <th>E12</th>\n",
       "      <th>E13</th>\n",
       "      <th>E14</th>\n",
       "      <th>E15</th>\n",
       "      <th>E16</th>\n",
       "      <th>E17</th>\n",
       "      <th>E18</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0xb2d8e1f9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xb2da4f54</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xb2d0f4e5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x3b9b3b70</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>9335.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xb2d8d086</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76165</th>\n",
       "      <td>0xb2d174f1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>50000000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>50000010.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76166</th>\n",
       "      <td>0x3b9af14f</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>370000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76167</th>\n",
       "      <td>0xb2d1bb5d</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76168</th>\n",
       "      <td>0xb2d9ed26</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76169</th>\n",
       "      <td>0xb2d3049f</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76170 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cust_no  label  bef_label  I3_x  I5  I6  I10  I11  I13  I14  ...  \\\n",
       "0      0xb2d8e1f9      1        1.0     3   4   0    3  0.0    2    5  ...   \n",
       "1      0xb2da4f54      1        NaN     0   0   0    3  0.0    2    5  ...   \n",
       "2      0xb2d0f4e5      1        1.0     0   7   0    3  0.0    2    5  ...   \n",
       "3      0x3b9b3b70      1        1.0     3   0   0    3  0.0    2    5  ...   \n",
       "4      0xb2d8d086      1        1.0     2   5   0    3  0.0    2    5  ...   \n",
       "...           ...    ...        ...   ...  ..  ..  ...  ...  ...  ...  ...   \n",
       "76165  0xb2d174f1      1       -1.0     1   5   0    3  0.0    2    5  ...   \n",
       "76166  0x3b9af14f      0        0.0     1   0   0    3  0.0    2    5  ...   \n",
       "76167  0xb2d1bb5d      0        1.0     3   5   0    3  0.0    2    5  ...   \n",
       "76168  0xb2d9ed26      1        NaN     0   1   0    3  0.0    2    5  ...   \n",
       "76169  0xb2d3049f      1        1.0     1   7   0    3  0.0    2    5  ...   \n",
       "\n",
       "       E9  E10  E12  E13  E14         E15  E16         E17  E18   c  \n",
       "0       5    1    5    5    5      1300.0    1         0.0    5 NaN  \n",
       "1       5    1    5    5    5         0.0    5         0.0    5 NaN  \n",
       "2       5    5    5    5    5         0.0    5         0.0    5 NaN  \n",
       "3       5    5    5    5    5    110000.0    5      9335.0    5 NaN  \n",
       "4       5    2    5    5    5         0.0    5    300000.0    2 NaN  \n",
       "...    ..  ...  ...  ...  ...         ...  ...         ...  ...  ..  \n",
       "76165   5    2    5    2    5  50000000.0    2  50000010.0    2 NaN  \n",
       "76166   5    5    5    5    5    200000.0    5    370000.0    5 NaN  \n",
       "76167   5    5    5    5    5     50000.0    3    120000.0    5 NaN  \n",
       "76168   5    1    5    5    5         0.0    5         0.0    5 NaN  \n",
       "76169   5    5    5    5    5     50000.0    5    200000.0    5 NaN  \n",
       "\n",
       "[76170 rows x 186 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加入权重信息，权重系数是直接参考大佬分享https://github.com/BirderEric/XianmenBank，猜想这个系数是通过暴力搜索得到的\n",
    "train['weight'] = train['label'].map({0:1.03,1:0.58,-1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_col =['bef_label'] + [f for f in train.columns if f.startswith('I') and f not in  ['I11'] ] + [f for f in train.columns if f.startswith('E') and f not in ['E15', 'E17']]\n",
    "# train.loc[(train['bef_label'].isnull())&(train['E1']==1),'bef_label'] =2\n",
    "# test.loc[(test['bef_label'].isnull())&(test['E1']==1),'bef_label'] =2\n",
    "# train.loc[(train['bef_label'].isnull()),'bef_label'] =3\n",
    "# test.loc[(test['bef_label'].isnull()),'bef_label'] =3\n",
    "train[cat_col] = train[cat_col].astype(np.int) \n",
    "test[cat_col] = test[cat_col].astype(np.int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['bef_label'].isnull()),'bef_label'] =3\n",
    "test.loc[(test['bef_label'].isnull()),'bef_label'] =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., -1.,  3.,  2.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['bef_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# train.to_pickle('train.pkl')\n",
    "# test.to_pickle('test.pkl')\n",
    "train = pd.read_pickle('train.pkl')\n",
    "test = pd.read_pickle('test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_gZdEMJNq1m"
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current num of features: 183\n",
      "(60936, 183) (15234, 183)\n",
      "['bef_label', 'I3_x', 'I5', 'I6', 'I10', 'I13', 'I14', 'I16', 'I20', 'I3_y', 'E1', 'E2', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E12', 'E13', 'E14', 'E16', 'E18'] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182]\n",
      "0:\tlearn: 0.4235962\ttest: 0.4220361\tbest: 0.4220361 (0)\ttotal: 605ms\tremaining: 25m 11s\n",
      "100:\tlearn: 0.4854554\ttest: 0.4795510\tbest: 0.4795510 (100)\ttotal: 49.6s\tremaining: 19m 39s\n",
      "200:\tlearn: 0.5047862\ttest: 0.4872705\tbest: 0.4872826 (199)\ttotal: 1m 36s\tremaining: 18m 19s\n",
      "300:\tlearn: 0.5230704\ttest: 0.4914439\tbest: 0.4924317 (299)\ttotal: 2m 26s\tremaining: 17m 47s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4936060427\n",
      "bestIteration = 329\n",
      "\n",
      "Shrink model to first 330 iterations.\n",
      "(60936, 183) (15234, 183)\n",
      "['bef_label', 'I3_x', 'I5', 'I6', 'I10', 'I13', 'I14', 'I16', 'I20', 'I3_y', 'E1', 'E2', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E12', 'E13', 'E14', 'E16', 'E18'] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182]\n",
      "0:\tlearn: 0.4387039\ttest: 0.4344931\tbest: 0.4344931 (0)\ttotal: 479ms\tremaining: 19m 56s\n",
      "100:\tlearn: 0.4874020\ttest: 0.4703072\tbest: 0.4706182 (99)\ttotal: 49.8s\tremaining: 19m 42s\n",
      "200:\tlearn: 0.5028714\ttest: 0.4796110\tbest: 0.4796427 (198)\ttotal: 1m 39s\tremaining: 18m 52s\n",
      "300:\tlearn: 0.5211000\ttest: 0.4822345\tbest: 0.4829562 (289)\ttotal: 2m 26s\tremaining: 17m 52s\n",
      "400:\tlearn: 0.5403889\ttest: 0.4857579\tbest: 0.4868824 (380)\ttotal: 3m 15s\tremaining: 17m 1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4868824239\n",
      "bestIteration = 380\n",
      "\n",
      "Shrink model to first 381 iterations.\n",
      "(60936, 183) (15234, 183)\n",
      "['bef_label', 'I3_x', 'I5', 'I6', 'I10', 'I13', 'I14', 'I16', 'I20', 'I3_y', 'E1', 'E2', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E12', 'E13', 'E14', 'E16', 'E18'] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182]\n",
      "0:\tlearn: 0.4290558\ttest: 0.3988471\tbest: 0.3988471 (0)\ttotal: 475ms\tremaining: 19m 48s\n",
      "100:\tlearn: 0.4916296\ttest: 0.4583068\tbest: 0.4583068 (100)\ttotal: 50s\tremaining: 19m 48s\n",
      "200:\tlearn: 0.5076784\ttest: 0.4656053\tbest: 0.4657785 (198)\ttotal: 1m 38s\tremaining: 18m 46s\n",
      "300:\tlearn: 0.5256109\ttest: 0.4696077\tbest: 0.4696077 (300)\ttotal: 2m 28s\tremaining: 18m 3s\n",
      "400:\tlearn: 0.5411798\ttest: 0.4721408\tbest: 0.4725856 (390)\ttotal: 3m 15s\tremaining: 17m 5s\n",
      "500:\tlearn: 0.5544147\ttest: 0.4726065\tbest: 0.4735761 (476)\ttotal: 4m 3s\tremaining: 16m 12s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.475227594\n",
      "bestIteration = 509\n",
      "\n",
      "Shrink model to first 510 iterations.\n",
      "(60936, 183) (15234, 183)\n",
      "['bef_label', 'I3_x', 'I5', 'I6', 'I10', 'I13', 'I14', 'I16', 'I20', 'I3_y', 'E1', 'E2', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E12', 'E13', 'E14', 'E16', 'E18'] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182]\n",
      "0:\tlearn: 0.4315351\ttest: 0.4077750\tbest: 0.4077750 (0)\ttotal: 477ms\tremaining: 19m 52s\n",
      "100:\tlearn: 0.4921895\ttest: 0.4609068\tbest: 0.4609068 (100)\ttotal: 48.3s\tremaining: 19m 7s\n",
      "200:\tlearn: 0.5108772\ttest: 0.4661000\tbest: 0.4672152 (169)\ttotal: 1m 35s\tremaining: 18m 11s\n",
      "300:\tlearn: 0.5280391\ttest: 0.4703947\tbest: 0.4710700 (297)\ttotal: 2m 23s\tremaining: 17m 28s\n",
      "400:\tlearn: 0.5452976\ttest: 0.4738628\tbest: 0.4742312 (399)\ttotal: 3m 9s\tremaining: 16m 31s\n",
      "500:\tlearn: 0.5578817\ttest: 0.4745765\tbest: 0.4756189 (488)\ttotal: 3m 55s\tremaining: 15m 40s\n",
      "600:\tlearn: 0.5702954\ttest: 0.4775224\tbest: 0.4793947 (568)\ttotal: 4m 41s\tremaining: 14m 50s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.479394655\n",
      "bestIteration = 568\n",
      "\n",
      "Shrink model to first 569 iterations.\n",
      "(60936, 183) (15234, 183)\n",
      "['bef_label', 'I3_x', 'I5', 'I6', 'I10', 'I13', 'I14', 'I16', 'I20', 'I3_y', 'E1', 'E2', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E12', 'E13', 'E14', 'E16', 'E18'] [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182]\n",
      "0:\tlearn: 0.4180132\ttest: 0.4214686\tbest: 0.4214686 (0)\ttotal: 481ms\tremaining: 20m 2s\n",
      "100:\tlearn: 0.4886499\ttest: 0.4760192\tbest: 0.4760192 (100)\ttotal: 48.3s\tremaining: 19m 6s\n",
      "200:\tlearn: 0.5062448\ttest: 0.4814220\tbest: 0.4814220 (200)\ttotal: 1m 37s\tremaining: 18m 34s\n",
      "300:\tlearn: 0.5248881\ttest: 0.4852454\tbest: 0.4865080 (279)\ttotal: 2m 25s\tremaining: 17m 39s\n",
      "400:\tlearn: 0.5412595\ttest: 0.4881680\tbest: 0.4890104 (388)\ttotal: 3m 12s\tremaining: 16m 47s\n",
      "500:\tlearn: 0.5537852\ttest: 0.4913967\tbest: 0.4915842 (496)\ttotal: 3m 59s\tremaining: 15m 57s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4915842392\n",
      "bestIteration = 496\n",
      "\n",
      "Shrink model to first 497 iterations.\n",
      "OOF-MEAN-KAPPA score:0.485339, OOF-STD:0.007033\n",
      "feature importance:\n",
      "feature\n",
      "X_sum_2                      6.317038\n",
      "X_sum_4                      6.067653\n",
      "X_sum_5                      4.944394\n",
      "X_sum_last_2_X_sum_last_1    4.638419\n",
      "X_huoqilv_last_2             4.297981\n",
      "I3_x                         4.188394\n",
      "X_sum_last_2                 3.917792\n",
      "X_sum_min_2                  3.115399\n",
      "C1_min_2                     3.012476\n",
      "bef_label                    2.489783\n",
      "B7_last_2                    2.348450\n",
      "X_huoqilv_sum_2              1.873124\n",
      "C1_max_2_C1_max_1            1.694619\n",
      "B6_last_2                    1.651987\n",
      "B7_last_1                    1.554240\n",
      "Name: importance, dtype: float64\n",
      "confusion matrix:\n",
      "[[ 6652  2318  2617]\n",
      " [ 1576  8168  5443]\n",
      " [ 1920  6400 41076]]\n",
      "classfication report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.57      0.61     11587\n",
      "          1       0.48      0.54      0.51     15187\n",
      "          2       0.84      0.83      0.83     49396\n",
      "\n",
      "avg / total       0.74      0.73      0.74     76170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import catboost as cat\n",
    "from catboost import Pool\n",
    "\n",
    "def kappa(preds, train_data):\n",
    "    y_true = train_data.label\n",
    "    preds = np.argmax(preds.reshape(3, -1), axis=0)\n",
    "    score = cohen_kappa_score(y_true, preds)\n",
    "    return 'kappa', score, True\n",
    "\n",
    "\n",
    "def Cat_classfication_model(train, target, test, k):\n",
    "#     cat_features = [i for i in stat_col if i in train.columns] + [i for i in train.columns[16:24]]\n",
    "    feats = [f for f in train.columns if f not in ['cust_no', 'label', 'weight']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = KFold(n_splits=5, shuffle=False, random_state=2020)\n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    oof_probs = np.zeros((train.shape[0], 3))\n",
    "    output_preds = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    offline_score = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "        train_weight, valid_weight = train['weight'][train_index], train['weight'][test_index]\n",
    "        print(train_X.shape, test_X.shape)\n",
    "        print(cat_col, [train_X.columns.get_loc(col) for col in cat_col])\n",
    "        train_data = Pool(data=train_X,\n",
    "                          label=train_y,\n",
    "                          cat_features=cat_col,\n",
    "                          weight=train_weight.values.flatten(order='F'),\n",
    "\n",
    "                         )\n",
    "        valid_data = Pool(data=test_X,\n",
    "                          label=test_y,\n",
    "                          cat_features=cat_col,\n",
    "                         )\n",
    "        cat_model =cat.CatBoostClassifier(iterations=2500, learning_rate=0.057, max_depth=7, l2_leaf_reg=2, verbose=100,\n",
    "                                       early_stopping_rounds=50,loss_function='MultiClass'\n",
    "                                          , eval_metric='Kappa',\n",
    "                                     )\n",
    "\n",
    "        cat_model.fit(train_data,\n",
    "          eval_set= valid_data,\n",
    "          use_best_model=True,\n",
    "         )\n",
    "        \n",
    "        oof_probs[test_index] = cat_model.predict_proba(test_X[feats])\n",
    "        oof_preds[test_index] = np.argmax(oof_probs[test_index], axis=1)\n",
    "        offline_score.append(cat_model.get_best_score()['validation']['Kappa'])\n",
    "        output_preds.append(cat_model.predict_proba(test[feats]))\n",
    "        \n",
    "        \n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = cat_model.get_feature_importance()\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-KAPPA score:%.6f, OOF-STD:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(15))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(target, oof_preds))\n",
    "    print('classfication report:')\n",
    "    print(classification_report(target, oof_preds))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score)\n",
    "target = train['label'] + 1\n",
    "cab_preds, cab_oof, cab_score = Cat_classfication_model(train, target,test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    0.620995\n",
      " 0    0.242668\n",
      "-1    0.136336\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sub_df = test[['cust_no']].copy()\n",
    "sub_df['label'] = np.argmax(np.mean(cab_preds, axis=0), axis=1) - 1\n",
    "sub_df.to_csv('cab_final.csv', index=False)\n",
    "print(sub_df['label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current num of features: 183\n",
      "(60936, 183) (15234, 183)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.498811\n",
      "[200]\tvalid_0's kappa: 0.497067\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's kappa: 0.499666\n",
      "(60936, 183) (15234, 183)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.496294\n",
      "[200]\tvalid_0's kappa: 0.490494\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's kappa: 0.497513\n",
      "(60936, 183) (15234, 183)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.47704\n",
      "[200]\tvalid_0's kappa: 0.479691\n",
      "[300]\tvalid_0's kappa: 0.479174\n",
      "[400]\tvalid_0's kappa: 0.478031\n",
      "Early stopping, best iteration is:\n",
      "[311]\tvalid_0's kappa: 0.481414\n",
      "(60936, 183) (15234, 183)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.478329\n",
      "[200]\tvalid_0's kappa: 0.480827\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's kappa: 0.482114\n",
      "(60936, 183) (15234, 183)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's kappa: 0.494341\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid_0's kappa: 0.495007\n",
      "OOF-MEAN-KAPPA score:0.491143, OOF-STD:0.007802\n",
      "feature importance:\n",
      "feature\n",
      "X_sum_min_2                  63396.307388\n",
      "X_sum_last_2_X_sum_last_1    41112.154120\n",
      "X_sum_4                      28759.417966\n",
      "X_sum_last_2                 21510.402716\n",
      "C1_min_2                     16035.050676\n",
      "X_huoqilv_last_2             13384.483530\n",
      "X_sum_5                       8432.119949\n",
      "B7_last_2                     8299.091704\n",
      "X3_last_2                     7645.733685\n",
      "bef_label                     7603.165403\n",
      "B6_last_2                     7589.804814\n",
      "B7_last_1                     6001.489836\n",
      "X_sum_2                       5949.130500\n",
      "X_fuzailv_last_2              5880.895373\n",
      "I3_x                          5535.950804\n",
      "Name: importance, dtype: float64\n",
      "confusion matrix:\n",
      "[[ 6730  2218  2639]\n",
      " [ 1572  8100  5515]\n",
      " [ 1913  6096 41387]]\n",
      "classfication report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.58      0.62     11587\n",
      "          1       0.49      0.53      0.51     15187\n",
      "          2       0.84      0.84      0.84     49396\n",
      "\n",
      "avg / total       0.74      0.74      0.74     76170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def kappa(preds, train_data):\n",
    "    y_true = train_data.label\n",
    "    preds = np.argmax(preds.reshape(3, -1), axis=0)\n",
    "    score = cohen_kappa_score(y_true, preds)\n",
    "    return 'kappa', score, True\n",
    "\n",
    "def LGB_classfication_model(train, target, test, k):\n",
    "    feats = [f for f in train.columns if f not in ['cust_no', 'label', 'weight']]\n",
    "    print('Current num of features:', len(feats))\n",
    "    folds = KFold(n_splits=k, shuffle=False, random_state=2020)\n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    oof_probs = np.zeros((train.shape[0], 3))\n",
    "    output_preds = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    offline_score = []\n",
    "    for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "        train_y, test_y = target[train_index], target[test_index]\n",
    "        train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "        train_weight, valid_weight = train['weight'][train_index], train['weight'][test_index]\n",
    "        print(train_X.shape, test_X.shape)\n",
    "        dtrain = lgb.Dataset(train_X,\n",
    "                             label=train_y,\n",
    "                             categorical_feature=cat_col, \n",
    "                             weight=train_weight.values.flatten(order='F'),\n",
    "                            )\n",
    "        dval = lgb.Dataset(test_X,\n",
    "                           label=test_y,\n",
    "                          categorical_feature=cat_col, )\n",
    "        parameters = {\n",
    "            'learning_rate': 0.05,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'None',\n",
    "            'num_leaves': 63,\n",
    "            'num_class': 3,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'verbose': -1,\n",
    "            'nthread': 24\n",
    "        }\n",
    "        lgb_model = lgb.train(\n",
    "            parameters,\n",
    "            dtrain,\n",
    "            num_boost_round=5000,\n",
    "            valid_sets=[dval],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=100,\n",
    "            feval=kappa, \n",
    "        )\n",
    "        oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration)\n",
    "        oof_preds[test_index] = np.argmax(lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration), axis=1)\n",
    "        offline_score.append(lgb_model.best_score['valid_0']['kappa'])\n",
    "            \n",
    "\n",
    "        output_preds.append(lgb_model.predict(test[feats], num_iteration=lgb_model.best_iteration))\n",
    "        # feature importance\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('OOF-MEAN-KAPPA score:%.6f, OOF-STD:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "    print('feature importance:')\n",
    "    print(feature_importance_df.groupby(['feature'])['importance'].mean().sort_values(ascending=False).head(15))\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(target, oof_preds))\n",
    "    print('classfication report:')\n",
    "    print(classification_report(target, oof_preds))\n",
    "\n",
    "    return output_preds, oof_probs, np.mean(offline_score)\n",
    "target = train['label'] + 1\n",
    "lgb_preds, lgb_oof, lgb_score = LGB_classfication_model(train, target, test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DGbxoIoduN9m"
   },
   "outputs": [],
   "source": [
    "sub_df = test[['cust_no']].copy()\n",
    "sub_df['label'] = np.argmax(np.mean(lgb_preds, axis=0), axis=1) - 1\n",
    "sub_df.to_csv('lgb_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9L_rlheXNq1m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     72509\n",
       "False     4213\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lgb_final = pd.read_csv(\"lgb_final.csv\")\n",
    "cab_final = pd.read_csv(\"cab_final.csv\")\n",
    "(lgb_final['label']==cab_final['label']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "p_gZdEMJNq1m",
    "0aZ7FUbfNq1m"
   ],
   "name": "lgb8.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
